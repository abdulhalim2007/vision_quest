{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad7b3369-46e1-4cae-925d-220211d52d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Dense, Dropout,\n",
    "    Flatten, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ea884b9-010a-4c9a-9e4d-7532eaae021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 48\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25\n",
    "NUM_CLASSES = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72747cc5-6eda-42b9-9903-78984f012bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "127af936-0b49-461c-8f5e-29a92b649209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = datagen.flow_from_directory(\n",
    "    \"data/train\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset=\"training\",\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88918786-ff41-432e-a52c-abb6f44dc4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5741 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data = datagen.flow_from_directory(\n",
    "    \"data/train\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset=\"validation\",\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec7ebc48-f201-4774-88b7-5364270cd263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "print(train_data.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1267f076-3098-4d70-9228-552aa179f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulhalim/cv/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu', input_shape=(48,48,1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c35ee3d7-e42f-457f-ba7a-90ff43f6d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c9e020e-1ee4-4d8e-a8b2-c75598cd08d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulhalim/cv/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 167ms/step - accuracy: 0.2168 - loss: 2.4828 - val_accuracy: 0.1636 - val_loss: 2.0234\n",
      "Epoch 2/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 173ms/step - accuracy: 0.3318 - loss: 1.7799 - val_accuracy: 0.3071 - val_loss: 1.7403\n",
      "Epoch 3/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 193ms/step - accuracy: 0.3948 - loss: 1.5833 - val_accuracy: 0.3930 - val_loss: 1.5448\n",
      "Epoch 4/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 198ms/step - accuracy: 0.4403 - loss: 1.4617 - val_accuracy: 0.3708 - val_loss: 1.7636\n",
      "Epoch 5/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 213ms/step - accuracy: 0.4587 - loss: 1.4117 - val_accuracy: 0.4393 - val_loss: 1.4614\n",
      "Epoch 6/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 232ms/step - accuracy: 0.4837 - loss: 1.3633 - val_accuracy: 0.4907 - val_loss: 1.3320\n",
      "Epoch 7/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 264ms/step - accuracy: 0.4863 - loss: 1.3412 - val_accuracy: 0.4055 - val_loss: 1.5936\n",
      "Epoch 8/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 270ms/step - accuracy: 0.4990 - loss: 1.3195 - val_accuracy: 0.4442 - val_loss: 1.4404\n",
      "Epoch 9/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 271ms/step - accuracy: 0.5079 - loss: 1.2924 - val_accuracy: 0.4147 - val_loss: 1.4781\n",
      "Epoch 10/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 281ms/step - accuracy: 0.5115 - loss: 1.2766 - val_accuracy: 0.5229 - val_loss: 1.2543\n",
      "Epoch 11/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 274ms/step - accuracy: 0.5281 - loss: 1.2654 - val_accuracy: 0.5048 - val_loss: 1.3057\n",
      "Epoch 12/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 233ms/step - accuracy: 0.5248 - loss: 1.2461 - val_accuracy: 0.4902 - val_loss: 1.3266\n",
      "Epoch 13/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 237ms/step - accuracy: 0.5339 - loss: 1.2336 - val_accuracy: 0.5276 - val_loss: 1.2430\n",
      "Epoch 14/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 229ms/step - accuracy: 0.5427 - loss: 1.2127 - val_accuracy: 0.4534 - val_loss: 1.4439\n",
      "Epoch 15/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 183ms/step - accuracy: 0.5416 - loss: 1.2167 - val_accuracy: 0.4961 - val_loss: 1.2783\n",
      "Epoch 16/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 152ms/step - accuracy: 0.5411 - loss: 1.2119 - val_accuracy: 0.4821 - val_loss: 1.3444\n",
      "Epoch 17/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 158ms/step - accuracy: 0.5519 - loss: 1.1905 - val_accuracy: 0.5440 - val_loss: 1.2227\n",
      "Epoch 18/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 174ms/step - accuracy: 0.5580 - loss: 1.1815 - val_accuracy: 0.5337 - val_loss: 1.2344\n",
      "Epoch 19/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 219ms/step - accuracy: 0.5592 - loss: 1.1789 - val_accuracy: 0.5071 - val_loss: 1.3223\n",
      "Epoch 20/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 204ms/step - accuracy: 0.5565 - loss: 1.1824 - val_accuracy: 0.4771 - val_loss: 1.3783\n",
      "Epoch 21/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 179ms/step - accuracy: 0.5624 - loss: 1.1647 - val_accuracy: 0.5619 - val_loss: 1.1648\n",
      "Epoch 22/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 213ms/step - accuracy: 0.5666 - loss: 1.1486 - val_accuracy: 0.4959 - val_loss: 1.3239\n",
      "Epoch 23/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 233ms/step - accuracy: 0.5688 - loss: 1.1421 - val_accuracy: 0.5337 - val_loss: 1.2459\n",
      "Epoch 24/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 236ms/step - accuracy: 0.5709 - loss: 1.1435 - val_accuracy: 0.5713 - val_loss: 1.1401\n",
      "Epoch 25/25\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 209ms/step - accuracy: 0.5775 - loss: 1.1395 - val_accuracy: 0.5292 - val_loss: 1.3035\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=25\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf723570-b753-4af7-aa24-a7df57632900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"vintage_night_emotion_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37d9ee56-f149-414e-b514-ddd7cf176522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Predicted Emotion: neutral\n"
     ]
    }
   ],
   "source": [
    "emotion_labels = [\n",
    "    \"Angry\", \"Disgust\", \"Fear\",\n",
    "    \"Happy\", \"neutral\", \"Sad\", \"Suprise\"\n",
    "]\n",
    "\n",
    "img = cv2.imread(\"data/train/neutral/Training_18498226.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (48,48))\n",
    "img = img / 255.0\n",
    "img = img.reshape(1,48,48,1)\n",
    "\n",
    "prediction = model.predict(img)\n",
    "emotion = emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "print(\"Predicted Emotion:\", emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1123c-12a8-4edb-871c-86e72a887e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
